# SWE-bench Verified (OpenAI Preparedness Team, 2024)

**논문/블로그**: "Introducing SWE-bench Verified"
**저자**: OpenAI Preparedness Team
**발표**: 2024

---

## 핵심 수치

| 항목 | 수치 |
|------|------|
| 검토 대상 샘플 | **1,699개** |
| 최종 선정 | **500개** |
| 필터링 비율 | **68.3%** |
| 어노테이터 수 | **93명** (Upwork 모집 전문 Python 개발자) |
| 불명확한 이슈 설명 비율 | **38.3%** |
| 부당한 테스트 케이스 비율 | **61.1%** |
| 공개 당시 최고 성능 (GPT-4o) | **33.2%** |
| 1년 반 후 최고 성능 | **80%+** |

---

## 난이도 분류

| 등급 | 기준 | 개수 |
|------|------|------|
| Easy | ≤15분 | 196개 |
| Medium | 15분~1시간 | 259개 |
| Hard | ≥1시간 | 45개 |

---

## 발견된 품질 문제 3가지

1. **Overly Specific Tests**: 특정 에러 메시지 문자열 일치 검사 등 과도하게 구체적
2. **Underspecified Issues**: 이슈 설명이 모호해서 PR 토론 없이는 해결책 특정 불가
3. **Environment Setup**: Deprecated API, 암묵적 의존성 버전 가정

---

## 어노테이션 기준

- **이슈 명확도**: 이슈만 보고 해결책을 특정할 수 있는가?
- **테스트 공정성**: 올바른 솔루션이 테스트를 통과하는가?
- 0~3점 심각도 스케일, 3중 교차 검증

---

## 인프라: Docker 환경 표준화

- SWE-bench 원저자와 협력하여 컨테이너화된 Docker 환경 구축
- 각 인스턴스별 독립 환경 → 의존성 충돌 방지
- 커뮤니티 표준 벤치마크로 자리잡음

---

## 시사점

- 원본 SWE-bench 점수가 실제 능력보다 **낮게** 측정되었을 가능성 (올바른 솔루션도 통과 못하는 경우)
- 500개 고품질 인스턴스가 2,294개 전체보다 더 신뢰할 수 있는 평가 기준
