# SWE-bench Pro (Deng et al., 2025)

**논문**: "SWE-Bench Pro: Can AI Agents Solve Long-Horizon SE Tasks?"
**저자**: Deng et al. (Scale AI)
**arXiv**: 2509.16941

---

## 핵심 수치

| 항목 | 수치 |
|------|------|
| Public 인스턴스 | **731개** |
| Held-out 인스턴스 | **858개** |
| Commercial 인스턴스 | **276개** |
| 전체 | **1,865개** (731+858+276) |
| Commercial 레포 수 | 18개 스타트업 비공개 코드 |
| 평균 패치 줄수 | **107.4줄** (최소 10줄 의무화) |
| 평균 수정 파일 수 | **4.1개** |
| SWE-bench Verified 대비 복잡도 | **~3배** (32.8줄/1.7파일 vs 107.4줄/4.1파일) |

---

## 3단계 데이터셋 구조

| 구분 | 개수 | 특징 |
|------|------|------|
| **Public** | 731개 | GPL Copyleft 라이선스 → 법적 위험으로 학습 억제 |
| **Held-out** | 858개 | 비공개 유지 → 미래 과적합 탐지용 |
| **Commercial** | 276개 | 18개 스타트업 비공개 코드 → 학습 불가능 |

---

## 주요 성능 결과

### SWE-Agent 설정 (Public/Commercial, 최대 50턴, $2 예산)

| 모델 | Public | Commercial |
|------|:------:|:----------:|
| Claude Sonnet 4.5 | **43.6%** | — |
| GPT-5 (high) | **41.8%** | **15.7%** |
| Claude Opus 4.1 | — | **17.8%** |
| Gemini 2.5 Pro | — | 10.1% |
| GPT-4o | — | 3.6% |

- GPT-5: Public 41.8% → Commercial 15.7% (−26.1%p 하락)
- Public vs Commercial 격차가 오염/일반화 수준을 반영

### 다른 에이전트 설정 (Held-out 기준)

- GPT-5: Public 23.1% → Private 14.9% (8.2%p 하락)

---

## 실패 원인 분석 (LLM-as-Judge)

슬라이드 기준 (arxiv 논문 최신 데이터):

| 실패 원인 | Claude Opus 4.1 | Claude Sonnet 4 | GPT-5 |
|----------|:--------------:|:--------------:|:-----:|
| 잘못된 솔루션 | **50.3%** | — | 39.5% |
| 문법 오류 | 31.3% | — | — |
| 컨텍스트 오버플로우 | — | **62.6%** | — |
| 무한 파일 읽기 | — | 57.4% | — |
| 도구 사용 실패 | — | — | 17.7% |

**공통 병목**: 컨텍스트 오버플로우 — 긴 코드베이스 처리 능력

---

## 핵심 혁신

- GPL 라이선스를 오염 방어막으로 활용 (폐쇄형 모델이 GPL 코드 훈련 시 법적 위험)
- Commercial Set: 스타트업 파트너십으로 미공개 코드 확보 (훈련 데이터에 절대 포함 불가)
- 인간 보강: 원본 이슈의 8.4~25.9% 누락 맥락을 추가로 해결

---

## 한계

- 비공개 Commercial Set으로 커뮤니티 재현 어려움
- Java, C++, Rust 언어 비중 낮음
- 에이전트 50턴, $2 비용 상한선
