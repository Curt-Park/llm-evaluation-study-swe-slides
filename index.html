<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM 코딩 벤치마크: 진화의 궤적과 남은 과제</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5.1.0/dist/reveal.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5.1.0/dist/theme/black.css">
    <link rel="stylesheet" href="css/custom.css">
</head>
<body>
<div class="reveal">
<div class="slides">

<!-- ============================================ -->
<!-- Section 1: 도입 (슬라이드 1-4) -->
<!-- ============================================ -->

<section>
    <h1 class="title-main">LLM 코딩 벤치마크</h1>
    <h2 class="title-sub">진화의 궤적과 남은 과제</h2>
    <p class="title-meta">Curt Park</p>
    <aside class="notes">
        1시간 발표. LLM 코딩 벤치마크가 어떻게 진화해왔고, 어떤 한계가 남아있는지 논문 데이터를 기반으로 살펴봅니다.
    </aside>
</section>

<section>
    <h2>Agenda</h2>
    <ol class="agenda-list">
        <li>SE 벤치마크 조감도</li>
        <li>출발점: HumanEval</li>
        <li>현실 세계로: SWE-bench</li>
        <li>Python을 넘어서: 다국어 확장</li>
        <li>불편한 진실: 오염 문제</li>
        <li>오염 대응 전략</li>
        <li>남은 과제와 전망</li>
    </ol>
    <aside class="notes">
        총 7개 섹션으로 구성. 벤치마크의 계보를 시간순으로 따라가며 각 단계의 동기와 한계를 분석합니다.
    </aside>
</section>

<section>
    <h2>왜 코딩 벤치마크인가?</h2>
    <ul>
        <li>LLM의 가장 활발한 응용 분야 = <strong>코딩</strong></li>
        <li>"측정할 수 없으면 개선할 수 없다"</li>
        <li>벤치마크 점수가 모델 선택과 투자를 결정</li>
        <li>그러나... 벤치마크 점수 = 실제 능력?</li>
    </ul>
    <aside class="notes">
        벤치마크가 단순한 학술 도구를 넘어 산업에 미치는 영향이 큽니다. 하지만 점수의 의미를 정확히 이해하려면 벤치마크의 설계와 한계를 알아야 합니다.
    </aside>
</section>

<section>
    <h2>SE 벤치마크 전체 조감도</h2>
    <p class="chart-note">소프트웨어 엔지니어링 벤치마크 291개 분석 (2025)</p>
    <div class="chart-container">
        <canvas id="chart-se-distribution"></canvas>
    </div>
    <p class="source">Source: Hou et al., 2025</p>
    <aside class="notes">
        SE 전체 291개 벤치마크 중 코딩이 43%로 가장 많습니다. 요구사항 공학, 설계, 프로젝트 관리 등은 10% 미만으로, 정량적 자동 평가가 어려운 영역입니다.
    </aside>
</section>

<!-- ============================================ -->
<!-- Section 2: HumanEval (슬라이드 5-10) -->
<!-- ============================================ -->

<section>
    <h2 class="section-title">출발점: HumanEval</h2>
    <p class="section-subtitle">체계적 코딩 벤치마크의 시작 (2021)</p>
</section>

<section>
    <h2>HumanEval이란?</h2>
    <div class="two-column">
        <div class="col">
            <h3>구조</h3>
            <ul>
                <li><strong>164</strong>개 Python 문제</li>
                <li>함수 시그니처 + 독스트링</li>
                <li>문제당 평균 <strong>7.7</strong>개 유닛 테스트</li>
                <li>자기완결적 함수 단위</li>
            </ul>
        </div>
        <div class="col">
            <h3>평가 메트릭: pass@k</h3>
            <ul>
                <li>k개 코드 생성 후</li>
                <li>하나라도 모든 테스트 통과하면 성공</li>
                <li>pass@1: 실용적 지표</li>
                <li>pass@100: 잠재력 지표</li>
            </ul>
        </div>
    </div>
    <aside class="notes">
        OpenAI가 Codex 논문과 함께 공개. 처음으로 코드 생성 능력을 체계적으로 측정할 수 있는 프레임워크를 제시했습니다.
    </aside>
</section>

<section>
    <h2>HumanEval 성능 추이</h2>
    <p class="chart-note">pass@1 (%) — 4년간의 진화</p>
    <div class="chart-container">
        <canvas id="chart-humaneval-timeline"></canvas>
    </div>
    <aside class="notes">
        2021년 Codex 28.8%에서 시작해 불과 4년 만에 96.3%까지 도달. 벤치마크가 사실상 포화 상태에 이르렀습니다.
    </aside>
</section>

<section>
    <h2>벤치마크 포화의 증거</h2>
    <div class="highlight-box warning">
        <h3>변형 태스크에서의 성능 하락</h3>
        <p>Top 모델들이 HumanEval 원본에서는 90%+ 달성하지만,<br>
        변형된 태스크에서는 <strong class="red">19.6 ~ 47.7%p</strong> 하락</p>
    </div>
    <ul>
        <li>구성적 일반화(compositional generalization) 부족</li>
        <li>문제를 "풀었다" vs 문제를 "외웠다"의 차이</li>
        <li>벤치마크 점수의 과대평가 가능성</li>
    </ul>
    <aside class="notes">
        단순히 높은 점수를 받는 것과 실제로 코딩 능력이 있는 것은 다릅니다. 변형 실험이 이를 잘 보여줍니다.
    </aside>
</section>

<section>
    <h2>HumanEval의 한계</h2>
    <table class="styled-table">
        <thead>
            <tr><th>한계</th><th>설명</th></tr>
        </thead>
        <tbody>
            <tr><td>범위</td><td>자기완결적 함수 단위만 평가</td></tr>
            <tr><td>언어</td><td>Python 단일 언어</td></tr>
            <tr><td>복잡도</td><td>코드베이스 전체 이해 불필요</td></tr>
            <tr><td>현실성</td><td>실제 개발 환경과 괴리</td></tr>
            <tr><td>오염</td><td>학습 데이터 오염 가능성 높음</td></tr>
        </tbody>
    </table>
    <p class="emphasis">→ 더 현실적인 벤치마크가 필요하다</p>
    <aside class="notes">
        이러한 한계들이 SWE-bench의 탄생 배경이 됩니다.
    </aside>
</section>

<section>
    <h2>HumanEval → SWE-bench</h2>
    <p class="emphasis">패러다임 전환의 필요성</p>
    <table class="styled-table comparison-table">
        <thead>
            <tr><th></th><th>HumanEval</th><th>SWE-bench</th></tr>
        </thead>
        <tbody>
            <tr><td>단위</td><td>함수</td><td>코드베이스 전체</td></tr>
            <tr><td>맥락</td><td>독스트링</td><td>실제 GitHub 이슈</td></tr>
            <tr><td>규모</td><td>164 문제</td><td>2,294 이슈</td></tr>
            <tr><td>현실성</td><td>알고리즘 퍼즐</td><td>실제 버그/기능</td></tr>
            <tr><td>난이도</td><td>Top 모델 96%+</td><td>Top 모델 ~20%</td></tr>
        </tbody>
    </table>
</section>

<!-- ============================================ -->
<!-- Section 3: SWE-bench (슬라이드 11-18) -->
<!-- ============================================ -->

<section>
    <h2 class="section-title">현실 세계로: SWE-bench</h2>
    <p class="section-subtitle">실제 GitHub 이슈 기반 평가 (2023)</p>
</section>

<section>
    <h2>SWE-bench 설계</h2>
    <div class="two-column">
        <div class="col">
            <h3>데이터 소스</h3>
            <ul>
                <li><strong>12</strong>개 인기 Python 레포</li>
                <li>Django, scikit-learn, matplotlib, sympy 등</li>
                <li><strong>2,294</strong>개 실제 GitHub 이슈</li>
                <li>이슈 → PR → 테스트의 자연스러운 흐름</li>
            </ul>
        </div>
        <div class="col">
            <h3>평가 방식</h3>
            <ul>
                <li>이슈 설명을 입력으로 제공</li>
                <li>코드베이스 전체 접근 가능</li>
                <li>기존 테스트 + 새 테스트로 검증</li>
                <li>실제 PR과 동일한 결과 기대</li>
            </ul>
        </div>
    </div>
    <aside class="notes">
        SWE-bench의 핵심은 실제 오픈소스 프로젝트의 이슈를 그대로 사용한다는 것입니다. 모델이 코드베이스 전체를 이해하고 수정해야 합니다.
    </aside>
</section>

<section>
    <h2>SWE-bench 초기 결과</h2>
    <div class="stat-grid">
        <div class="stat-card">
            <div class="stat-number">12.47%</div>
            <div class="stat-label">SWE-agent + GPT-4<br>(최초 기준선)</div>
        </div>
        <div class="stat-card">
            <div class="stat-number">~20%</div>
            <div class="stat-label">2025년 초<br>최고 성능</div>
        </div>
        <div class="stat-card">
            <div class="stat-number">2,294</div>
            <div class="stat-label">전체 태스크 수</div>
        </div>
    </div>
    <p class="emphasis">Full SWE-bench에서 20%도 대단한 발전</p>
    <aside class="notes">
        HumanEval에서 96%를 달성하는 모델도 SWE-bench에서는 고작 20% 수준. 실제 소프트웨어 엔지니어링의 어려움을 보여줍니다.
    </aside>
</section>

<section>
    <h2>SWE-bench의 품질 문제</h2>
    <ul>
        <li>2,294개 중 상당수가 <strong>모호</strong>하거나 <strong>불명확</strong></li>
        <li>테스트 사양이 불충분한 경우 존재</li>
        <li>올바른 솔루션도 테스트 실패 가능</li>
        <li>벤치마크 자체의 신뢰성 의문 제기</li>
    </ul>
    <p class="emphasis">→ 품질 검증된 부분집합이 필요하다</p>
</section>

<section>
    <h2>SWE-bench Verified</h2>
    <div class="highlight-box info">
        <h3>전문가 검증 프로세스</h3>
        <ul>
            <li><strong>93</strong>명의 전문 개발자 참여</li>
            <li><strong>1,699</strong>개 샘플에 대해 3중 교차 검증</li>
            <li>난이도/품질 가이드라인에 따른 평가</li>
            <li>최종 <strong>500</strong>개 고품질 인스턴스 선정</li>
        </ul>
    </div>
    <aside class="notes">
        OpenAI와 협력하여 만든 검증된 하위 집합. 벤치마크 자체의 품질을 보장하는 중요한 시도입니다.
    </aside>
</section>

<section>
    <h2>SWE-bench Verified 리더보드</h2>
    <p class="chart-note">해결률 (%) — 2026년 2월 기준</p>
    <div class="chart-container">
        <canvas id="chart-swebench-verified"></canvas>
    </div>
    <aside class="notes">
        Verified에서는 상위 모델이 80%를 돌파. Full에서 20%였던 것과 극적인 대비. Verified 기준으로 모델 간 차이가 더 잘 드러납니다.
    </aside>
</section>

<section>
    <h2>Verified vs Full의 의미</h2>
    <table class="styled-table">
        <thead>
            <tr><th></th><th>SWE-bench Full</th><th>SWE-bench Verified</th></tr>
        </thead>
        <tbody>
            <tr><td>크기</td><td>2,294</td><td>500</td></tr>
            <tr><td>Top 성능</td><td>~20%</td><td>~81%</td></tr>
            <tr><td>품질 보장</td><td>자동화</td><td>전문가 3중 검증</td></tr>
            <tr><td>주요 용도</td><td>도전적 평가</td><td>모델 비교</td></tr>
        </tbody>
    </table>
    <div class="highlight-box warning">
        <p>같은 "SWE-bench"라도 어떤 버전인지에 따라<br>점수의 의미가 완전히 다르다</p>
    </div>
</section>

<section>
    <h2>SWE-bench의 남은 한계</h2>
    <div class="highlight-box warning">
        <h3>Python Only</h3>
        <p>12개 레포 모두 Python<br>
        실제 소프트웨어 세계는 다양한 언어로 구성</p>
    </div>
    <p class="emphasis">→ 다국어 벤치마크의 필요성</p>
</section>

<!-- ============================================ -->
<!-- Section 4: 다국어 확장 (슬라이드 19-24) -->
<!-- ============================================ -->

<section>
    <h2 class="section-title">Python을 넘어서</h2>
    <p class="section-subtitle">다국어 벤치마크의 등장</p>
</section>

<section>
    <h2>Multi-SWE-bench</h2>
    <p class="source">ByteDance, 2025</p>
    <div class="two-column">
        <div class="col">
            <h3>규모</h3>
            <ul>
                <li><strong>8</strong>개 언어</li>
                <li><strong>2,132</strong>개 인스턴스</li>
                <li><strong>68</strong>명 어노테이터</li>
                <li>5단계 검증 파이프라인</li>
            </ul>
        </div>
        <div class="col">
            <h3>지원 언어</h3>
            <ul>
                <li>Python, Java, TypeScript</li>
                <li>JavaScript, Go, Rust</li>
                <li>C, C++</li>
            </ul>
        </div>
    </div>
</section>

<section>
    <h2>SWE-bench Multilingual</h2>
    <p class="source">SWE-bench 원저자팀</p>
    <div class="two-column">
        <div class="col">
            <h3>규모</h3>
            <ul>
                <li><strong>9</strong>개 언어</li>
                <li><strong>42</strong>개 레포지토리</li>
                <li><strong>300</strong>개 큐레이션 태스크</li>
                <li>반자동 + 수동 검증 결합</li>
            </ul>
        </div>
        <div class="col">
            <h3>지원 언어</h3>
            <ul>
                <li>C, C++, Go, Java</li>
                <li>JavaScript, TypeScript</li>
                <li>PHP, Ruby, Rust</li>
            </ul>
        </div>
    </div>
</section>

<section>
    <h2>Python vs 다국어 성능 격차</h2>
    <p class="chart-note">SWE-bench Verified (Python) vs Multilingual 해결률 (%)</p>
    <div class="chart-container">
        <canvas id="chart-multilingual"></canvas>
    </div>
    <aside class="notes">
        Claude 3.7 Sonnet 기준으로 Python에서 63%, Multilingual에서 43%로 무려 20%p 격차. LLM이 Python에 치우쳐 학습되어 있음을 보여줍니다.
    </aside>
</section>

<section>
    <h2>다국어 벤치마크의 발견</h2>
    <ul>
        <li><strong>Python 편향</strong>: 대부분의 모델이 Python에서 월등히 높은 성능</li>
        <li><strong>크로스 파일 이슈</strong>에서 성능 급락</li>
        <li>JS/TS 패치는 상대적으로 <strong>지역적</strong> (<3 hunks, <2 files)</li>
        <li>난이도가 올라갈수록 해결률 급격히 하락</li>
    </ul>
    <div class="highlight-box info">
        <p>LLM의 코딩 능력은 <strong>언어에 따라 크게 다르다</strong><br>
        Python 기준 점수로 일반화하면 안 된다</p>
    </div>
</section>

<section>
    <h2>다국어 벤치마크 비교</h2>
    <table class="styled-table">
        <thead>
            <tr><th></th><th>Multi-SWE-bench</th><th>SWE-bench Multilingual</th></tr>
        </thead>
        <tbody>
            <tr><td>주체</td><td>ByteDance</td><td>SWE-bench 원저자</td></tr>
            <tr><td>언어 수</td><td>8</td><td>9</td></tr>
            <tr><td>인스턴스</td><td>2,132</td><td>300</td></tr>
            <tr><td>검증</td><td>68명, 5단계</td><td>반자동 + 수동</td></tr>
            <tr><td>특징</td><td>대규모</td><td>빠른 평가 목적</td></tr>
        </tbody>
    </table>
</section>

<!-- ============================================ -->
<!-- Section 5: 오염 문제 (슬라이드 25-32) -->
<!-- ============================================ -->

<section>
    <h2 class="section-title">불편한 진실</h2>
    <p class="section-subtitle">데이터 오염 문제</p>
</section>

<section>
    <h2>The SWE-Bench Illusion</h2>
    <p class="source">Liang et al., Microsoft Research — NeurIPS 2025</p>
    <div class="highlight-box danger">
        <h3>핵심 질문</h3>
        <p>모델이 벤치마크를 정말 <strong>"풀고"</strong> 있는가,<br>
        아니면 <strong>"기억해내고"</strong> 있는가?</p>
    </div>
    <aside class="notes">
        Microsoft Research가 NeurIPS 2025에서 발표한 논문. SWE-bench 점수의 신뢰성에 근본적인 의문을 제기합니다.
    </aside>
</section>

<section>
    <h2>핵심 실험: 파일 경로 맞추기</h2>
    <ul>
        <li>모델에게 <strong>이슈 설명만</strong> 제공 (코드 접근 불가)</li>
        <li>버그가 있는 파일의 경로를 예측하도록 요청</li>
        <li>코드를 보지 않고도 경로를 맞출 수 있다면?</li>
    </ul>
    <div class="stat-grid">
        <div class="stat-card green">
            <div class="stat-number">76%</div>
            <div class="stat-label">SWE-bench Verified<br>파일 경로 정확도</div>
        </div>
        <div class="stat-card red">
            <div class="stat-number">53%</div>
            <div class="stat-label">외부 레포지토리<br>파일 경로 정확도</div>
        </div>
        <div class="stat-card yellow">
            <div class="stat-number">23%p</div>
            <div class="stat-label">성능 격차</div>
        </div>
    </div>
    <aside class="notes">
        코드를 보지도 않고 76% 정확도로 파일 경로를 맞춘다는 것은, 모델이 이미 이 벤치마크의 정답을 "알고 있다"는 강력한 증거입니다.
    </aside>
</section>

<section>
    <h2>오염도 그래디언트</h2>
    <p class="chart-note">파일 경로 식별 정확도 (%) — 데이터 소스별</p>
    <div class="chart-container">
        <canvas id="chart-contamination"></canvas>
    </div>
    <aside class="notes">
        오염도가 가장 높은 Verified에서 가장 높은 정확도, 외부 레포에서 가장 낮은 정확도. 깨끗한 그래디언트가 오염을 증명합니다.
    </aside>
</section>

<section>
    <h2>두 가지 암기 패턴</h2>
    <div class="two-column">
        <div class="col">
            <div class="highlight-box warning">
                <h3>인스턴스별 암기</h3>
                <p>특정 이슈-솔루션 쌍을<br>학습 데이터에서 기억</p>
                <hr>
                <p class="small">증거: Verified > Full > External<br>단계적 성능 하락</p>
            </div>
        </div>
        <div class="col">
            <div class="highlight-box warning">
                <h3>레포 편향 암기</h3>
                <p>특정 레포의 구조와<br>패턴을 과적합</p>
                <hr>
                <p class="small">증거: SWE-bench 레포 vs 외부 레포<br>최대 <strong>47%p</strong> 격차</p>
            </div>
        </div>
    </div>
</section>

<section>
    <h2>리더보드 경쟁의 함의</h2>
    <ul>
        <li>높은 SWE-bench 점수 ≠ 높은 범용 코딩 능력</li>
        <li>모델 학습 시 벤치마크 레포 데이터가 포함될 수밖에 없음</li>
        <li>신규 모델일수록 더 많은 데이터에 노출 → <strong>점수 인플레이션</strong></li>
        <li>공정한 비교를 위한 새로운 접근 필요</li>
    </ul>
    <div class="highlight-box danger">
        <p>벤치마크의 가치는 <strong>오염으로부터의 자유도</strong>에 비례한다</p>
    </div>
</section>

<!-- ============================================ -->
<!-- Section 6: 오염 대응 전략 (슬라이드 33-40) -->
<!-- ============================================ -->

<section>
    <h2 class="section-title">오염 대응 전략</h2>
    <p class="section-subtitle">주기적 갱신 vs 접근 제한</p>
</section>

<section>
    <h2>두 가지 접근법</h2>
    <div class="two-column">
        <div class="col">
            <div class="highlight-box info">
                <h3>전략 A: 주기적 갱신</h3>
                <p>지속적으로 새 태스크 생성하여<br>오염 창을 최소화</p>
                <hr>
                <ul>
                    <li>SWE-rebench</li>
                    <li>SWE-bench Live</li>
                </ul>
            </div>
        </div>
        <div class="col">
            <div class="highlight-box info">
                <h3>전략 B: 접근 제한</h3>
                <p>학습 데이터에 포함될 수 없는<br>코드를 사용</p>
                <hr>
                <ul>
                    <li>SWE-bench Pro</li>
                </ul>
            </div>
        </div>
    </div>
</section>

<section>
    <h2>SWE-rebench</h2>
    <p class="source">자동화된 대규모 갱신</p>
    <div class="stat-grid">
        <div class="stat-card">
            <div class="stat-number">3,400+</div>
            <div class="stat-label">레포지토리</div>
        </div>
        <div class="stat-card">
            <div class="stat-number">21,000+</div>
            <div class="stat-label">태스크</div>
        </div>
        <div class="stat-card">
            <div class="stat-number">$0.03</div>
            <div class="stat-label">최저 비용/문제</div>
        </div>
    </div>
    <ul>
        <li>자동화 파이프라인으로 지속적 태스크 수집</li>
        <li>오염 추적 메커니즘 내장</li>
        <li>비용/토큰 추적으로 효율성까지 평가</li>
    </ul>
</section>

<section>
    <h2>SWE-bench Live</h2>
    <p class="source">월간 갱신 방식</p>
    <div class="stat-grid">
        <div class="stat-card">
            <div class="stat-number">~50</div>
            <div class="stat-label">월간 신규 태스크</div>
        </div>
        <div class="stat-card">
            <div class="stat-number">1,565+</div>
            <div class="stat-label">누적 인스턴스</div>
        </div>
        <div class="stat-card">
            <div class="stat-number">164</div>
            <div class="stat-label">레포지토리</div>
        </div>
    </div>
    <ul>
        <li>모델 학습 컷오프 <strong>이후</strong> 생성된 이슈만 사용</li>
        <li>자동화된 월간 큐레이션 파이프라인</li>
        <li>시간 경과에 따른 모델 능력 추이 관찰 가능</li>
    </ul>
</section>

<section>
    <h2>SWE-bench Pro</h2>
    <p class="source">접근 제한 모델</p>
    <div class="two-column">
        <div class="col">
            <div class="highlight-box info">
                <h3>Public Split</h3>
                <ul>
                    <li><strong>731</strong>개 인스턴스</li>
                    <li>GPL Copyleft 라이선스</li>
                    <li>학습 시 법적 위험 → 억제 효과</li>
                </ul>
            </div>
        </div>
        <div class="col">
            <div class="highlight-box danger">
                <h3>Private Split</h3>
                <ul>
                    <li><strong>276</strong>개 인스턴스</li>
                    <li>18개 비공개 스타트업 코드</li>
                    <li>학습 데이터에 포함 <strong>불가능</strong></li>
                </ul>
            </div>
        </div>
    </div>
</section>

<section>
    <h2>Public vs Private: 현실 격차</h2>
    <p class="chart-note">SWE-bench Pro 해결률 (%) — Public vs Private</p>
    <div class="chart-container">
        <canvas id="chart-swebench-pro"></canvas>
    </div>
    <aside class="notes">
        Private 코드에서의 성능 하락이 모델의 "진짜 실력"에 더 가까울 수 있습니다. GPT-5의 경우 8.2%p 하락.
    </aside>
</section>

<section>
    <h2>대응 전략 트레이드오프</h2>
    <table class="styled-table">
        <thead>
            <tr><th></th><th>주기적 갱신</th><th>접근 제한</th></tr>
        </thead>
        <tbody>
            <tr><td>대표</td><td>SWE-rebench, Live</td><td>SWE-bench Pro</td></tr>
            <tr><td>장점</td><td>대규모, 자동화</td><td>오염 근본 차단</td></tr>
            <tr>
                <td>단점</td>
                <td class="red">개별 샘플 품질 저하</td>
                <td class="red">라이선스 일관성 문제</td>
            </tr>
            <tr>
                <td>한계</td>
                <td>갱신 주기 내 오염 가능</td>
                <td>새 기술 패러다임 반영 어려움</td>
            </tr>
        </tbody>
    </table>
    <p class="emphasis">두 전략 모두 완벽하지 않다 — 상호보완적 접근 필요</p>
</section>

<!-- ============================================ -->
<!-- Section 7: 남은 과제 (슬라이드 41-44) -->
<!-- ============================================ -->

<section>
    <h2 class="section-title">남은 과제와 전망</h2>
    <p class="section-subtitle">벤치마크 너머의 소프트웨어 엔지니어링</p>
</section>

<section>
    <h2>SWEBOK V4와 현실의 괴리</h2>
    <p class="chart-note">SWEBOK V4: 18개 지식 영역 — 벤치마크 커버리지</p>
    <div class="swebok-grid">
        <div class="swebok-item covered">소프트웨어 구현(Construction)</div>
        <div class="swebok-item covered">소프트웨어 테스팅</div>
        <div class="swebok-item partial">소프트웨어 유지보수</div>
        <div class="swebok-item partial">소프트웨어 보안</div>
        <div class="swebok-item uncovered">요구사항 공학</div>
        <div class="swebok-item uncovered">소프트웨어 설계</div>
        <div class="swebok-item uncovered">소프트웨어 아키텍처</div>
        <div class="swebok-item uncovered">형상 관리</div>
        <div class="swebok-item uncovered">소프트웨어 품질</div>
        <div class="swebok-item uncovered">소프트웨어 공학 운영</div>
        <div class="swebok-item uncovered">소프트웨어 공학 관리</div>
        <div class="swebok-item uncovered">소프트웨어 공학 프로세스</div>
        <div class="swebok-item uncovered">소프트웨어 공학 모델/방법론</div>
        <div class="swebok-item uncovered">소프트웨어 공학 전문실무</div>
        <div class="swebok-item uncovered">소프트웨어 공학 경제학</div>
        <div class="swebok-item uncovered">컴퓨팅 기초</div>
        <div class="swebok-item uncovered">수학 기초</div>
        <div class="swebok-item uncovered">공학 기초</div>
    </div>
    <div class="swebok-legend">
        <span class="legend-item covered">벤치마크 존재</span>
        <span class="legend-item partial">부분적 평가</span>
        <span class="legend-item uncovered">벤치마크 부재</span>
    </div>
    <aside class="notes">
        SWEBOK V4 기준 18개 지식 영역 중 벤치마크가 존재하는 것은 극소수. 현재 벤치마크는 소프트웨어 엔지니어링의 극히 일부만 평가하고 있습니다.
    </aside>
</section>

<section>
    <h2>벤치마크 진화의 교훈</h2>
    <div class="timeline-vertical">
        <div class="timeline-item">
            <div class="timeline-year">2021</div>
            <div class="timeline-content">
                <strong>HumanEval</strong> — 함수 단위, Python<br>
                <span class="small">→ 한계: 포화, 비현실적</span>
            </div>
        </div>
        <div class="timeline-item">
            <div class="timeline-year">2023</div>
            <div class="timeline-content">
                <strong>SWE-bench</strong> — 실제 이슈, 코드베이스<br>
                <span class="small">→ 한계: Python 단일 언어, 품질</span>
            </div>
        </div>
        <div class="timeline-item">
            <div class="timeline-year">2024</div>
            <div class="timeline-content">
                <strong>Verified / Multilingual</strong> — 품질, 다국어<br>
                <span class="small">→ 한계: 오염 문제 부각</span>
            </div>
        </div>
        <div class="timeline-item">
            <div class="timeline-year">2025</div>
            <div class="timeline-content">
                <strong>rebench / Live / Pro</strong> — 오염 대응<br>
                <span class="small">→ 한계: 자동화-품질 균형</span>
            </div>
        </div>
    </div>
</section>

<section>
    <h2>핵심 메시지</h2>
    <div class="key-messages">
        <div class="key-message">
            <span class="msg-number">1</span>
            <p>벤치마크 점수는 <strong>종합적 SW 엔지니어링 역량</strong>을 대변하지 않는다</p>
        </div>
        <div class="key-message">
            <span class="msg-number">2</span>
            <p><strong>자동화와 품질</strong>의 균형은 벤치마크 설계의 근본적 딜레마다</p>
        </div>
        <div class="key-message">
            <span class="msg-number">3</span>
            <p><strong>에이전틱 코딩 시대</strong>의 평가 방식은 아직 정립되지 않았다</p>
        </div>
    </div>
    <aside class="notes">
        세 가지 핵심 메시지를 강조하며 마무리합니다.
    </aside>
</section>

<!-- ============================================ -->
<!-- Section 8: Q&A (슬라이드 45) -->
<!-- ============================================ -->

<section>
    <h2>참고문헌</h2>
    <div class="references">
        <ol>
            <li>Chen et al., "Evaluating Large Language Models Trained on Code" (HumanEval), 2021</li>
            <li>Jimenez et al., "SWE-bench: Can Language Models Resolve Real-World GitHub Issues?", 2023</li>
            <li>OpenAI, "Introducing SWE-bench Verified", 2024</li>
            <li>Hou et al., "Large Language Models for Software Engineering: A Systematic Literature Review", 2025</li>
            <li>Liu et al., "Multi-SWE-bench: A Multilingual Benchmark for Real-World Code Fixing", 2025</li>
            <li>Zan et al., "SWE-bench Multilingual", 2025</li>
            <li>Liang et al., "The SWE-Bench Illusion", NeurIPS 2025</li>
            <li>Qi et al., "SWE-rebench: Automated Benchmark Construction for SWE", 2025</li>
            <li>Fourrier et al., "SWE-bench Live: A Contamination-Free Dynamic Benchmark", 2025</li>
            <li>Scale AI, "SWE-bench Pro", 2025</li>
            <li>IEEE CS, "SWEBOK V4", 2024</li>
        </ol>
    </div>
</section>

<section>
    <h1>Q&A</h1>
    <p class="title-meta">감사합니다</p>
</section>

</div>
</div>

<!-- Slide Preview Bar -->
<div id="preview-bar">
    <div id="preview-indicator">
        <span id="preview-current">1</span> / <span id="preview-total">1</span>
        <input type="number" id="preview-goto" min="1" placeholder="#" title="페이지 번호 입력 후 Enter">
    </div>
    <div id="preview-track"></div>
</div>

<script src="https://cdn.jsdelivr.net/npm/reveal.js@5.1.0/dist/reveal.js"></script>
<script src="https://cdn.jsdelivr.net/npm/reveal.js@5.1.0/plugin/notes/notes.js"></script>
<script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.7/dist/chart.umd.min.js"></script>
<script src="js/charts.js"></script>
<script>
    Reveal.initialize({
        hash: true,
        slideNumber: true,
        plugins: [RevealNotes],
        transition: 'slide',
        width: 1280,
        height: 720,
    });

    Reveal.on('slidechanged', event => {
        initChartsOnSlide(event.currentSlide);
        updatePreviewBar();
    });
    Reveal.on('ready', event => {
        initChartsOnSlide(event.currentSlide);
        buildPreviewBar();
    });

    function buildPreviewBar() {
        const track = document.getElementById('preview-track');
        const slides = document.querySelectorAll('.reveal .slides > section');
        const total = slides.length;
        track.innerHTML = '';

        document.getElementById('preview-total').textContent = total;
        document.getElementById('preview-goto').max = total;

        slides.forEach((slide, i) => {
            const thumb = document.createElement('div');
            thumb.className = 'preview-thumb';
            if (i === 0) thumb.classList.add('active');

            // Page number badge
            const badge = document.createElement('span');
            badge.className = 'preview-badge';
            badge.textContent = i + 1;
            thumb.appendChild(badge);

            // Clone slide content into a scaled-down preview
            const inner = document.createElement('div');
            inner.className = 'preview-inner';
            inner.innerHTML = slide.innerHTML;
            inner.querySelectorAll('.notes, aside').forEach(el => el.remove());
            thumb.appendChild(inner);

            thumb.addEventListener('click', () => {
                Reveal.slide(i);
            });

            track.appendChild(thumb);
        });

        // Drag-to-scroll on preview track
        let isDragging = false;
        let startX, scrollLeft;

        track.addEventListener('mousedown', (e) => {
            if (e.target.closest('.preview-thumb')) {
                // Allow click, start drag tracking
            }
            isDragging = true;
            track.classList.add('dragging');
            startX = e.pageX - track.offsetLeft;
            scrollLeft = track.scrollLeft;
            e.preventDefault();
        });

        track.addEventListener('mousemove', (e) => {
            if (!isDragging) return;
            const x = e.pageX - track.offsetLeft;
            const walk = (x - startX) * 1.5;
            track.scrollLeft = scrollLeft - walk;
        });

        const stopDrag = () => {
            isDragging = false;
            track.classList.remove('dragging');
        };
        track.addEventListener('mouseup', stopDrag);
        track.addEventListener('mouseleave', stopDrag);

        // Click after drag prevention
        let dragDistance = 0;
        track.addEventListener('mousedown', (e) => { dragDistance = 0; });
        track.addEventListener('mousemove', () => { if (isDragging) dragDistance++; });
        track.querySelectorAll('.preview-thumb').forEach((thumb, i) => {
            thumb.addEventListener('click', (e) => {
                if (dragDistance > 5) {
                    e.stopImmediatePropagation();
                }
            }, true);
        });
    }

    function updatePreviewBar() {
        const current = Reveal.getIndices().h;
        document.getElementById('preview-current').textContent = current + 1;
        const thumbs = document.querySelectorAll('.preview-thumb');
        thumbs.forEach((t, i) => {
            t.classList.toggle('active', i === current);
        });
        const active = document.querySelector('.preview-thumb.active');
        if (active) {
            active.scrollIntoView({ behavior: 'smooth', inline: 'center', block: 'nearest' });
        }
    }

    // Go-to page input
    document.getElementById('preview-goto').addEventListener('keydown', (e) => {
        if (e.key === 'Enter') {
            const page = parseInt(e.target.value);
            const total = document.querySelectorAll('.reveal .slides > section').length;
            if (page >= 1 && page <= total) {
                Reveal.slide(page - 1);
            }
            e.target.value = '';
            e.target.blur();
        }
        e.stopPropagation(); // prevent Reveal.js key handling
    });
    document.getElementById('preview-goto').addEventListener('keyup', (e) => e.stopPropagation());
</script>
</body>
</html>
